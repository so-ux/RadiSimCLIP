## RadiSimCLIP: A Radiology Vision-Language Model Pretrained on Simulated Radiologist Learning Dataset for Zero-Shot Medical Image Understanding


We have constructed RadiSim, a radiology-specific dataset with 10.6M multi- source image-text pairs. Using RadiSim, RadiSimCLIP achieves SOTA accuracy on 15/16 tasks via hierarchical validation: 1. anatomical recognition via zero-shot classification, 2. pixel-level understanding through zero-shot detection, and 3. semantic alignment via cross-modal retrieval. This demonstrates general-purpose AI’s potential for specialized medical imaging.

The data distribution of the RadiSim dataset and its partial data processing workflow are shown below.
![Dataset](Images/dataset.png)

Overview of RadiSimCLIP training and three typical downstream tasks.
![downsteam task](Images/down_task.png)


### Abstract
Medical vision-language models (MVLMs) have shown significant promise in medical diagnosis. However, most existing approaches are constrained by limited modality coverage and often rely on datasets lacking real-world radiological diversity and semantic precision. Moreover, current models exhibit insufficient evaluation on fine-grained tasks. In this paper, we introduce RadiSim, a large-scale Radiology dataset comprising 10.6 million image-text pairs from CT, MRI, and DR modalities, designed to Simulate the learning resources during radiologist training. Building on RadiSim, we propose RadiSimCLIP, a radiology-specific MVLM pretrained using a CLIP-style contrastive framework. The model progressively acquires multilevel capabilities—ranging from anatomical recognition and pixel-level interpretability to vision-language alignment—mirroring how radiologists gain expertise. To systematically evaluate generalizability, we further propose a three-part evaluation framework: (1) zero-shot classification, which includes a novel extension to 3D volumetric data, evaluating anatomical, modality, and disease recognition; (2) zero-shot organ detection, which evaluates pixel-level semantic localization without parameter tuning; and (3) cross-modal retrieval, measuring alignment between visual features and clinical text. RadiSimCLIP achieves state-of-the-art results on 15 of 16 down-stream benchmarks. This work underscores the feasibility and potential of large-scale radiology-specific MVLMs for robust, zero-shot 3D medical understanding, representing a practical step toward clinically applicable foundation models.


- `src/`: core training and inference code
- `configs/`: experiment and model configurations
- `scripts/`: data preparation, training, and evaluation scripts
- `Images/`: figures and visualizations

This section will be updated once code is released.

### Environment
- Python >= 3.9 (recommended: 3.11)
- PyTorch >= 2.2 (match with your CUDA version)
- openc-clip-torch == 2.24
- Common packages: `tqdm`, `numpy`, `scikit-learn`, `einops`, `matplotlib`

Installation:
```bash
conda create -n radisimclip python=3.11 -y
conda activate radisimclip
pip install -r requirements.txt
```

### Data and Preparation
RadiSim is composed of heterogeneous radiology data intended to simulate the learning resources used during radiologist training.

Data organization (example):
```
data/
  ct/
  mri/
  dr/
```

Download links (to be completed):

| Dataset | Modality | Scale | Download | License |
| --- | --- | --- | --- | --- |
| RadiSim-CT | CT | <# images / pairs> | <link> | <license> |
| RadiSim-MRI | MRI | <# images / pairs> | <link> | <license> |
| RadiSim-DR | DR | <# images / pairs> | <link> | <license> |
| RadiSim-All | Mixed | 10.6M pairs | <link> | <license> |

Preprocessing (example): resampling, intensity normalization, anonymization, slicing/patching for 3D volumes.

### Training and Evaluation
Training:
```bash
python src/train.py \
  --config configs/base.yaml \
  --data_root data/ \
  --output_dir outputs/run_001
```

Evaluation:
```bash
python src/eval.py \
  --checkpoint outputs/run_001/ckpt_best.pt \
  --data_root data/
```

Distributed training (optional):
```bash
torchrun --nproc_per_node=4 src/train.py --config configs/base.yaml
```

Zero-shot evaluation includes: (1) classification (anatomy, modality, disease; with a 3D volumetric extension), (2) organ detection via pixel-level localization without finetuning, and (3) cross-modal retrieval.

### Reproducibility
- Pretrained weights: <download link(s)>
- Seeds and configs: provide `configs/*.yaml` and note random seeds used
- Primary metrics: report Top-1 / ROC-AUC / mAP as applicable

### Checkpoints
| Model | Pretraining Data | Params | Link |
| --- | --- | --- | --- |
| RadiSimCLIP-B | RadiSim (10.6M) | <#M> | <ckpt link> |
| RadiSimCLIP-L | RadiSim (10.6M) | <#M> | <ckpt link> |

### Citation
If you find this work useful, please cite:
```bibtex
@article{radisimclip2025,
  title   = {RadiSimCLIP: A Radiology Vision-Language Model Pretrained on Simulated Radiologist Learning Dataset for Zero-Shot Medical Image Understanding},
  author  = {<Author list>},
  journal = {<Venue>},
  year    = {2025},
  doi     = {<DOI>},
  url     = {<URL>}
}
```

### License
Recommended options: MIT / Apache-2.0 / CC BY-NC 4.0 (to be confirmed).

### Acknowledgements
We thank collaborators, data contributors, and supporting organizations.

---

Maintainer: <name / email / homepage>
Issues and contributions: please open an Issue or Pull Request