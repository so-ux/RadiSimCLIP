## RadiSimCLIP: A Radiology Vision-Language Model Pretrained on Simulated Radiologist Learning Dataset for Zero-Shot Medical Image Understanding

One-line summary: Official repository for the RadiSimCLIP paper, including environment setup, figures, data links, and reproducibility instructions.

### Paper Information
- Title: RadiSimCLIP: A Radiology Vision-Language Model Pretrained on Simulated Radiologist Learning Dataset for Zero-Shot Medical Image Understanding
- Authors: <Author list>
- Affiliations: <Institution(s)>
- Venue & Year: <Conference/Journal, Year>
- Paper Links: <arXiv/DOI/Project page>

### Abstract
Medical vision-language models (MVLMs) have shown significant promise in medical diagnosis. However, most existing approaches are constrained by limited modality coverage and often rely on datasets lacking real-world radiological diversity and semantic precision. Moreover, current models exhibit insufficient evaluation on fine-grained tasks. In this paper, we introduce RadiSim, a large-scale Radiology dataset comprising 10.6 million image-text pairs from CT, MRI, and DR modalities, designed to Simulate the learning resources during radiologist training. Building on RadiSim, we propose RadiSimCLIP, a radiology-specific MVLM pretrained using a CLIP-style contrastive framework. The model progressively acquires multilevel capabilities—ranging from anatomical recognition and pixel-level interpretability to vision-language alignment—mirroring how radiologists gain expertise. To systematically evaluate generalizability, we further propose a three-part evaluation framework: (1) zero-shot classification, which includes a novel extension to 3D volumetric data, evaluating anatomical, modality, and disease recognition; (2) zero-shot organ detection, which evaluates pixel-level semantic localization without parameter tuning; and (3) cross-modal retrieval, measuring alignment between visual features and clinical text. RadiSimCLIP achieves state-of-the-art results on 15 of 16 down-stream benchmarks. This work underscores the feasibility and potential of large-scale radiology-specific MVLMs for robust, zero-shot 3D medical understanding, representing a practical step toward clinically applicable foundation models.

### Highlights
- Method: CLIP-style contrastive pretraining tailored for radiology across CT, MRI, and DR.
- Contributions: 10.6M RadiSim dataset; progressive multilevel capabilities; a unified three-part zero-shot evaluation protocol with a 3D extension.
- Results: State-of-the-art performance on 15/16 downstream benchmarks covering classification, detection, and retrieval.

### Planned Repository Structure
- `src/`: core training and inference code
- `configs/`: experiment and model configurations
- `scripts/`: data preparation, training, and evaluation scripts
- `assets/`: figures and visualizations

This section will be updated once code is released.

### Environment
- Python >= 3.9 (recommended: 3.10)
- PyTorch >= 2.1 (match with your CUDA version)
- Common packages: `tqdm`, `numpy`, `scikit-learn`, `einops`, `matplotlib`

Installation:
```bash
conda create -n radisimclip python=3.10 -y
conda activate radisimclip
pip install -r requirements.txt
```

If needed, we will generate `requirements.txt` from the released code.

### Figures
Please place figures under `assets/figs/` and reference them below. Example placeholders:

![Overview](assets/figs/overview.png)
![Training Pipeline](assets/figs/training_pipeline.png)
![Benchmarks](assets/figs/benchmarks.png)

### Data and Preparation
RadiSim is composed of heterogeneous radiology data intended to simulate the learning resources used during radiologist training.

Data organization (example):
```
data/
  ct/
  mri/
  dr/
```

Download links (to be completed):

| Dataset | Modality | Scale | Download | License |
| --- | --- | --- | --- | --- |
| RadiSim-CT | CT | <# images / pairs> | <link> | <license> |
| RadiSim-MRI | MRI | <# images / pairs> | <link> | <license> |
| RadiSim-DR | DR | <# images / pairs> | <link> | <license> |
| RadiSim-All | Mixed | 10.6M pairs | <link> | <license> |

Preprocessing (example): resampling, intensity normalization, anonymization, slicing/patching for 3D volumes.

### Training and Evaluation
Training:
```bash
python src/train.py \
  --config configs/base.yaml \
  --data_root data/ \
  --output_dir outputs/run_001
```

Evaluation:
```bash
python src/eval.py \
  --checkpoint outputs/run_001/ckpt_best.pt \
  --data_root data/
```

Distributed training (optional):
```bash
torchrun --nproc_per_node=4 src/train.py --config configs/base.yaml
```

Zero-shot evaluation includes: (1) classification (anatomy, modality, disease; with a 3D volumetric extension), (2) organ detection via pixel-level localization without finetuning, and (3) cross-modal retrieval.

### Reproducibility
- Pretrained weights: <download link(s)>
- Seeds and configs: provide `configs/*.yaml` and note random seeds used
- Primary metrics: report Top-1 / ROC-AUC / mAP as applicable

### Checkpoints
| Model | Pretraining Data | Params | Link |
| --- | --- | --- | --- |
| RadiSimCLIP-B | RadiSim (10.6M) | <#M> | <ckpt link> |
| RadiSimCLIP-L | RadiSim (10.6M) | <#M> | <ckpt link> |

### Citation
If you find this work useful, please cite:
```bibtex
@article{radisimclip2025,
  title   = {RadiSimCLIP: A Radiology Vision-Language Model Pretrained on Simulated Radiologist Learning Dataset for Zero-Shot Medical Image Understanding},
  author  = {<Author list>},
  journal = {<Venue>},
  year    = {2025},
  doi     = {<DOI>},
  url     = {<URL>}
}
```

### License
Recommended options: MIT / Apache-2.0 / CC BY-NC 4.0 (to be confirmed).

### Acknowledgements
We thank collaborators, data contributors, and supporting organizations.

---

Maintainer: <name / email / homepage>
Issues and contributions: please open an Issue or Pull Request