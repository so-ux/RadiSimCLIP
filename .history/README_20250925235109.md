## RadiSimCLIP: A Radiology Vision-Language Model Pretrained on Simulated Radiologist Learning Dataset for Zero-Shot Medical Image Understanding

[![arXiv](https://img.shields.io/badge/MedAGI-2025.09-b31b1b.svg?style=for-the-badge)](https://medagi2025.github.io/) [![github](https://img.shields.io/badge/Github-RadiSimCLIP-orange)](https://github.com/so-ux/RadiSimCLIP)
<a href="#LICENSE--citation"><img alt="License: Apache-2.0 license" src="https://img.shields.io/badge/LICENSE-Apache 2.0-blue.svg"/></a>

We have constructed RadiSim, a radiology-specific dataset with 10.6M multi- source image-text pairs. Using RadiSim, RadiSimCLIP achieves SOTA accuracy on 15/16 tasks via a three-part hierarchical evaluation. Each part is summarized below:

#### 1) Zero-shot Anatomical Classification
- Goal: assess anatomical, modality, and disease recognition without finetuning.
- Inputs: 2D radiographs and 3D volumetric slices/volumes (CT, MRI, DR).
- Protocol: prompt-based text templates; 3D extension via volume-wise aggregation.
- Metrics: Top-1 / Top-5 accuracy, ROC-AUC where applicable.

#### 2) Zero-shot Pixel-level Detection
- Goal: evaluate pixel-level semantic localization of organs/lesions without parameter updates.
- Approach: class-aware attention or gradient-based localization derived from the pretrained model.
- Outputs: heatmaps or bounding boxes for organ/disease regions.
- Metrics: Dice/IoU, detection mAP (as applicable to each benchmark).

#### 3) Cross-modal Retrieval
- Goal: measure vision‚Äìlanguage alignment via bidirectional retrieval.
- Tasks: image-to-text and text-to-image retrieval on clinical captions and reports.
- Metrics: Recall@K (R@1/5/10), mAP.

This demonstrates general-purpose AI‚Äôs potential for specialized medical imaging.

The data distribution of the RadiSim dataset and its partial data processing workflow are shown below.
![Dataset](Images/dataset.png)

Overview of RadiSimCLIP training and three typical downstream tasks.
![downsteam task](Images/down_task.png)

### Abstract

Medical vision-language models (MVLMs) have shown significant promise in medical diagnosis. However, most existing approaches are constrained by limited modality coverage and often rely on datasets lacking real-world radiological diversity and semantic precision. Moreover, current models exhibit insufficient evaluation on fine-grained tasks. In this paper, we introduce RadiSim, a large-scale Radiology dataset comprising 10.6 million image-text pairs from CT, MRI, and DR modalities, designed to Simulate the learning resources during radiologist training. Building on RadiSim, we propose RadiSimCLIP, a radiology-specific MVLM pretrained using a CLIP-style contrastive framework. The model progressively acquires multilevel capabilities‚Äîranging from anatomical recognition and pixel-level interpretability to vision-language alignment‚Äîmirroring how radiologists gain expertise. To systematically evaluate generalizability, we further propose a three-part evaluation framework: (1) zero-shot classification, which includes a novel extension to 3D volumetric data, evaluating anatomical, modality, and disease recognition; (2) zero-shot organ detection, which evaluates pixel-level semantic localization without parameter tuning; and (3) cross-modal retrieval, measuring alignment between visual features and clinical text. RadiSimCLIP achieves state-of-the-art results on 15 of 16 down-stream benchmarks. This work underscores the feasibility and potential of large-scale radiology-specific MVLMs for robust, zero-shot 3D medical understanding, representing a practical step toward clinically applicable foundation models.

- `src/`: core training and inference code
- `configs/`: experiment and model configurations
- `scripts/`: data preparation, training, and evaluation scripts
- `Images/`: figures and visualizations

This section will be updated once code is released.

### Environment

- Python >= 3.9 (recommended: 3.11)
- PyTorch >= 2.2 (match with your CUDA version)
- openc-clip-torch == 2.24
- Common packages: `tqdm`, `numpy`, `scikit-learn`, `einops`, `matplotlib`

## ‚ö°Ô∏è Installation

```bash
conda create -n radisimclip python=3.11 -y
conda activate radisimclip
pip install -r requirements.txt
```

### Data and Preparation

RadiSim is composed of heterogeneous radiology data intended to simulate the learning resources used during radiologist training.

| Dataset | Modality | Scale | Download | License |
| --- | --- | --- | --- | --- |
|MediMeTA | CT | 1645 | https://zenodo.org/records/7884735 | Open Access |
|RSNA Pneumonia | X-ray | 16428 | https://www.rsna.org/rsnai/ai-image-challenge/rsna-pneumonia-detection-challenge-2018 | Open Access |
|AMOS | CT&MR | 500 CT and 100 MRI | https://www.rsna.org/rsnai/ai-image-challenge/rsna-pneumonia-detection-challenge-2018 | Open Access | 
| PMC-OA | - | - | https://huggingface.co/datasets/axiong/pmc_oa_beta | Open Access |
| PubMed OA | - | - | https://pmc.ncbi.nlm.nih.gov/tools/openftlist/ | Open Access |
| VQA-RAD | - | - | https://osf.io/89kps/| Open Access |
| SLAKE | - | - | https://www.med-vqa.com/slake/ | Open Access |

### üöÄ Training and Evaluation

Training:

```bash
python src/train.py \
  --config configs/base.yaml \
  --data_root data/ \
  --output_dir outputs/run_001
```

Evaluation:

```bash
python src/eval.py \
  --checkpoint outputs/run_001/ckpt_best.pt \
  --data_root data/
```

Distributed training (optional):

```bash
torchrun --nproc_per_node=4 src/train.py --config configs/base.yaml
```

Zero-shot evaluation includes: (1) classification (anatomy, modality, disease; with a 3D volumetric extension), (2) organ detection via pixel-level localization without finetuning, and (3) cross-modal retrieval.

Performances of zero-shot classification on different modalities and tasks.
![Dataset](Images/ret_cls.png)

 Performance of cross-modal retrieval.
![downsteam task](Images/ret_retrieval.png)

### Checkpoints

| Model | Pretraining Data | Params | Link |
| --- | --- | --- | --- |
| BiomedCLIP | PMC-15M | <https://github.com/microsoft/BiomedCLIP_data_pipeline> | <https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224> |
| PMC-CLIP | pmc-oa | <https://github.com/WeixiongLin/PMC-CLIP> | <https://huggingface.co/datasets/axiong/pmc_oa/tree/main> |
| PubMedCLIP | pmc-oa | <https://github.com/sarahESL/PubMedCLIP> | <https://huggingface.co/sarahESL/PubMedCLIP/tree/maintree/main> |

### Citation

If the code, paper and weights help your research, please cite:

```
@InProceedings{ RadiSimClip_MedAGI_2025,
                 author = { Minhui Tan, Qingxia Wu, Boyang Zhang, Genqiang Ren, Jianlong Nie, Zhong Xue, Xiaohuan Cao(‚úâ), and Dinggang Shen(‚úâ) },
                 title = { { RadiSimCLIP: A Radiology Vision-Language Model Pretrained on Simulated Radiologist Learning Dataset for Zero-Shot Medical Image Understanding } }, 
                 booktitle = {MICCAI 2025 3rd International Workshop on Foundation Models for General Medical AI},
                 year = {2025},
                 publisher = {Springer Nature Switzerland},
                 month = {October},
              }
```

### Acknowledgements

Some codes are borrowed from [openclip](https://github.com/mlfoundations/open_clip)
We thank collaborators, data contributors, and supporting organizations.

---

## License

This project is released under the Apache 2.0 license. Please see the [LICENSE](LICENSE) file for more information.
